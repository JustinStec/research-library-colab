{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Academic Library with GTE-Qwen2-1.5B\n",
    "\n",
    "This notebook embeds your 1,260 library texts + 140 close readings using a 32K context model.\n",
    "\n",
    "**Before running:**\n",
    "1. Go to Runtime → Change runtime type → Select **A100 GPU** (or T4 if unavailable)\n",
    "2. Run the SQL in `add_vector_search.sql` in your Supabase SQL Editor first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install dependencies + clear stale model cache\n!pip install -q \"transformers>=4.44,<5\" sentence-transformers torch\n!pip install -q supabase==2.11.0 postgrest==0.19.0\n!rm -rf ~/.cache/huggingface/modules/transformers_modules/Alibaba*\n!rm -rf ~/.cache/huggingface/hub/models--Alibaba-NLP--gte-Qwen2-1.5B-instruct"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "SUPABASE_URL = \"https://zknmvifnbrycjwckkggy.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inprbm12aWZuYnJ5Y2p3Y2trZ2d5Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTcwMjM3OSwiZXhwIjoyMDg1Mjc4Mzc5fQ.NbKHG7-VKYSdCbvhSW9a1v-5OoLSVevyEKin_RI4pvQ\"\n",
    "\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "MAX_TOKENS = 32000  # Leave some buffer from 32768\n",
    "EMBEDDING_DIM = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Initialize Supabase client + fix vector column dimensions\nimport requests as http_requests\nfrom supabase import create_client\n\nsupabase = create_client(SUPABASE_URL, SUPABASE_KEY)\nprint(\"Connected to Supabase\")\n\n# The embedding column was created for 768 dims but GTE-Qwen2-1.5B outputs 1536.\n# Alter both tables to use the correct dimension.\nMIGRATION_SQL = \"\"\"\nALTER TABLE library_texts\n  ALTER COLUMN embedding TYPE vector(1536)\n  USING embedding::vector(1536);\n\nALTER TABLE close_readings\n  ALTER COLUMN embedding TYPE vector(1536)\n  USING embedding::vector(1536);\n\"\"\"\n\n# Also need to recreate the search functions with the new dimension\nFUNCTION_SQL = \"\"\"\nCREATE OR REPLACE FUNCTION library_semantic_search(\n  query_embedding vector(1536),\n  match_count int DEFAULT 10\n)\nRETURNS TABLE (\n  id uuid,\n  file_name text,\n  author text,\n  title text,\n  year int,\n  category text,\n  similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    lt.id,\n    lt.file_name,\n    lt.author,\n    lt.title,\n    lt.year,\n    lt.category,\n    1 - (lt.embedding <=> query_embedding) AS similarity\n  FROM library_texts lt\n  WHERE lt.embedding IS NOT NULL\n  ORDER BY lt.embedding <=> query_embedding\n  LIMIT match_count;\nEND;\n$$;\n\nCREATE OR REPLACE FUNCTION readings_semantic_search(\n  query_embedding vector(1536),\n  match_count int DEFAULT 10\n)\nRETURNS TABLE (\n  id uuid,\n  file_name text,\n  source_author text,\n  source_title text,\n  similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n  RETURN QUERY\n  SELECT\n    cr.id,\n    cr.file_name,\n    cr.source_author,\n    cr.source_title,\n    1 - (cr.embedding <=> query_embedding) AS similarity\n  FROM close_readings cr\n  WHERE cr.embedding IS NOT NULL\n  ORDER BY cr.embedding <=> query_embedding\n  LIMIT match_count;\nEND;\n$$;\n\"\"\"\n\nprint(\"Migrating vector columns from 768 → 1536 dimensions...\")\nfor sql in [MIGRATION_SQL, FUNCTION_SQL]:\n    resp = http_requests.post(\n        f\"{SUPABASE_URL}/rest/v1/rpc/exec_sql\",\n        headers={\n            \"apikey\": SUPABASE_KEY,\n            \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n            \"Content-Type\": \"application/json\",\n        },\n        json={\"query\": sql},\n        timeout=30,\n    )\n    if resp.status_code in (200, 204):\n        print(\"  OK\")\n    elif \"does not exist\" in resp.text:\n        # exec_sql RPC doesn't exist — print SQL for manual execution\n        print(\"\\n⚠ Cannot run SQL via API. Paste this into Supabase SQL Editor:\\n\")\n        print(sql)\n        print(\"=\" * 60)\n    else:\n        # May already be 1536 — check the error\n        if \"1536\" in resp.text:\n            print(\"  Already at 1536 dimensions.\")\n        else:\n            print(f\"  Response: {resp.status_code} {resp.text[:200]}\")\n            print(\"\\n⚠ If this failed, paste the SQL below into Supabase SQL Editor:\\n\")\n            print(sql)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load the embedding model using sentence-transformers\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\nprint(f\"Loading {MODEL_NAME}...\")\nprint(\"This may take a few minutes to download (~3GB)...\")\n\n# Qwen2 is natively supported in transformers 4.40+ — no custom code needed\nmodel = SentenceTransformer(MODEL_NAME, trust_remote_code=False)\nmodel.max_seq_length = MAX_TOKENS\n\n# Move to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\nprint(f\"Model loaded on {device}\")\nprint(f\"Max sequence length: {model.max_seq_length}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Embedding function with mean-pooling for long texts\nimport numpy as np\nimport gc\nimport os\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Use a conservative chunk size that fits in A100 VRAM alongside the model\n# 8K tokens * 4 chars/token = 32K chars per chunk\nCHUNK_TOKENS = 8000\nCHUNK_CHARS = CHUNK_TOKENS * 4\nOVERLAP_CHARS = 1000\n\ndef get_embedding(text: str) -> list:\n    \"\"\"Embed text, chunking and mean-pooling for long texts.\"\"\"\n\n    approx_tokens = len(text) // 4\n\n    if approx_tokens <= CHUNK_TOKENS:\n        embedding = model.encode(text, normalize_embeddings=True)\n        return embedding.tolist()\n\n    # Split into manageable chunks\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = min(start + CHUNK_CHARS, len(text))\n        chunks.append(text[start:end])\n        if end >= len(text):\n            break\n        start = end - OVERLAP_CHARS\n\n    # Embed one chunk at a time to avoid OOM\n    embeddings = []\n    for chunk in chunks:\n        emb = model.encode(chunk, normalize_embeddings=True)\n        embeddings.append(emb)\n        torch.cuda.empty_cache()\n\n    stacked = np.stack(embeddings, axis=0)\n    mean_embedding = np.mean(stacked, axis=0)\n    mean_embedding = mean_embedding / np.linalg.norm(mean_embedding)\n\n    return mean_embedding.tolist()\n\n# Test\ntest_emb = get_embedding(\"This is a test sentence about poetry and cognition.\")\nprint(f\"Embedding dimension: {len(test_emb)}\")\nprint(f\"Chunk size: {CHUNK_TOKENS} tokens ({CHUNK_CHARS} chars)\")\nprint(\"Embedding function works!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Fetch all library texts\n",
    "print(\"Fetching library texts...\")\n",
    "\n",
    "# Fetch in batches to avoid timeout\n",
    "all_library_texts = []\n",
    "batch_size = 100\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    response = supabase.table(\"library_texts\") \\\n",
    "        .select(\"id, file_name, content\") \\\n",
    "        .is_(\"embedding\", \"null\") \\\n",
    "        .range(offset, offset + batch_size - 1) \\\n",
    "        .execute()\n",
    "    \n",
    "    if not response.data:\n",
    "        break\n",
    "    \n",
    "    all_library_texts.extend(response.data)\n",
    "    offset += batch_size\n",
    "    print(f\"  Fetched {len(all_library_texts)} texts...\")\n",
    "\n",
    "print(f\"\\nTotal library texts to embed: {len(all_library_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Fetch all close readings\n",
    "print(\"Fetching close readings...\")\n",
    "\n",
    "all_readings = []\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    response = supabase.table(\"close_readings\") \\\n",
    "        .select(\"id, file_name, content\") \\\n",
    "        .is_(\"embedding\", \"null\") \\\n",
    "        .range(offset, offset + batch_size - 1) \\\n",
    "        .execute()\n",
    "    \n",
    "    if not response.data:\n",
    "        break\n",
    "    \n",
    "    all_readings.extend(response.data)\n",
    "    offset += batch_size\n",
    "\n",
    "print(f\"Total close readings to embed: {len(all_readings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Embed library texts\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "print(f\"\\nEmbedding {len(all_library_texts)} library texts...\")\n",
    "print(\"This will take a while. Progress is saved after each text.\\n\")\n",
    "\n",
    "library_errors = []\n",
    "\n",
    "for i, text_record in enumerate(tqdm(all_library_texts)):\n",
    "    try:\n",
    "        content = text_record.get(\"content\", \"\")\n",
    "        if not content or len(content.strip()) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Get embedding\n",
    "        embedding = get_embedding(content)\n",
    "        \n",
    "        # Update database\n",
    "        supabase.table(\"library_texts\") \\\n",
    "            .update({\"embedding\": embedding}) \\\n",
    "            .eq(\"id\", text_record[\"id\"]) \\\n",
    "            .execute()\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if (i + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        library_errors.append({\"file\": text_record.get(\"file_name\"), \"error\": str(e)})\n",
    "        print(f\"\\nError on {text_record.get('file_name')}: {e}\")\n",
    "\n",
    "print(f\"\\nLibrary embedding complete!\")\n",
    "print(f\"Errors: {len(library_errors)}\")\n",
    "if library_errors:\n",
    "    for err in library_errors[:10]:\n",
    "        print(f\"  - {err['file']}: {err['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Embed close readings\n",
    "print(f\"\\nEmbedding {len(all_readings)} close readings...\\n\")\n",
    "\n",
    "reading_errors = []\n",
    "\n",
    "for i, reading in enumerate(tqdm(all_readings)):\n",
    "    try:\n",
    "        content = reading.get(\"content\", \"\")\n",
    "        if not content or len(content.strip()) < 100:\n",
    "            continue\n",
    "        \n",
    "        embedding = get_embedding(content)\n",
    "        \n",
    "        supabase.table(\"close_readings\") \\\n",
    "            .update({\"embedding\": embedding}) \\\n",
    "            .eq(\"id\", reading[\"id\"]) \\\n",
    "            .execute()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        reading_errors.append({\"file\": reading.get(\"file_name\"), \"error\": str(e)})\n",
    "        print(f\"\\nError on {reading.get('file_name')}: {e}\")\n",
    "\n",
    "print(f\"\\nClose readings embedding complete!\")\n",
    "print(f\"Errors: {len(reading_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verify embeddings\n",
    "print(\"Verification:\")\n",
    "\n",
    "# Count embedded library texts\n",
    "lib_count = supabase.table(\"library_texts\") \\\n",
    "    .select(\"id\", count=\"exact\") \\\n",
    "    .not_.is_(\"embedding\", \"null\") \\\n",
    "    .execute()\n",
    "print(f\"Library texts with embeddings: {lib_count.count}\")\n",
    "\n",
    "# Count embedded readings\n",
    "read_count = supabase.table(\"close_readings\") \\\n",
    "    .select(\"id\", count=\"exact\") \\\n",
    "    .not_.is_(\"embedding\", \"null\") \\\n",
    "    .execute()\n",
    "print(f\"Close readings with embeddings: {read_count.count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Test semantic search\n",
    "print(\"\\nTesting semantic search...\\n\")\n",
    "\n",
    "test_query = \"the unity of thought and feeling in metaphysical poetry\"\n",
    "query_embedding = get_embedding(test_query)\n",
    "\n",
    "# Search library\n",
    "results = supabase.rpc(\n",
    "    \"library_semantic_search\",\n",
    "    {\"query_embedding\": query_embedding, \"match_count\": 5}\n",
    ").execute()\n",
    "\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "print(\"Top 5 library results:\")\n",
    "for r in results.data:\n",
    "    print(f\"  [{r['similarity']:.3f}] {r['author']} - {r['title']}\")\n",
    "\n",
    "# Search readings\n",
    "results = supabase.rpc(\n",
    "    \"readings_semantic_search\",\n",
    "    {\"query_embedding\": query_embedding, \"match_count\": 5}\n",
    ").execute()\n",
    "\n",
    "print(\"\\nTop 5 close reading results:\")\n",
    "for r in results.data:\n",
    "    print(f\"  [{r['similarity']:.3f}] {r['source_author']} - {r['source_title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Your library and close readings now have semantic embeddings. \n",
    "\n",
    "Next: Update your MCP server to include semantic search tools that use these embeddings."
   ]
  }
 ]
}