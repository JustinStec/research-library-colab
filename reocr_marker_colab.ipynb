{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustinStec/research-library-colab/blob/main/reocr_marker_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpcEK65MdmTZ"
      },
      "source": [
        "# Re-OCR Library with Marker + Citation Metadata Enrichment\n",
        "\n",
        "**What this does:**\n",
        "1. Downloads PDFs from Supabase Storage\n",
        "2. Re-OCRs all PDFs with marker (GPU-accelerated)\n",
        "3. Cleans extracted text (ligatures, smart quotes, OCR artifacts)\n",
        "4. Enriches bibliographic metadata via CrossRef API\n",
        "5. Uploads everything back to Supabase (sets `embedding = NULL` to trigger re-embedding)\n",
        "\n",
        "**Before running:**\n",
        "1. **Runtime → Change runtime type → A100 GPU + High RAM**\n",
        "2. Run `add_citation_metadata.sql` in **Supabase SQL Editor** to add citation columns (already in clipboard)\n",
        "3. Run cells in order\n",
        "\n",
        "**After running:**\n",
        "- Run `embed_library_colab.ipynb` to re-embed all texts with GTE-Qwen2-1.5B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MiHmvaoAdmTZ",
        "outputId": "421471ff-2694-474f-a741-da57bb696568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.7/195.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.9/189.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.63.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install -q marker-pdf requests tqdm\n",
        "!pip install -q supabase==2.11.0 postgrest==0.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h9oSdkEgdmTZ",
        "outputId": "935afede-11f0-4a39-dc1f-9a09da43c2e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set.\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Configuration\n",
        "import os\n",
        "\n",
        "SUPABASE_URL = \"https://zknmvifnbrycjwckkggy.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inprbm12aWZuYnJ5Y2p3Y2trZ2d5Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTcwMjM3OSwiZXhwIjoyMDg1Mjc4Mzc5fQ.NbKHG7-VKYSdCbvhSW9a1v-5OoLSVevyEKin_RI4pvQ\"\n",
        "\n",
        "BUCKET_NAME = \"pdf-library\"\n",
        "PDF_DIR = \"/content/pdfs\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "CHECKPOINT_PATH = \"/content/reocr_checkpoint.json\"\n",
        "CROSSREF_CHECKPOINT = \"/content/crossref_checkpoint.json\"\n",
        "\n",
        "# Polite CrossRef usage: include your email\n",
        "CROSSREF_MAILTO = \"jts3et@virginia.edu\"\n",
        "\n",
        "# Set CUDA for marker\n",
        "os.environ[\"TORCH_DEVICE\"] = \"cuda\"\n",
        "\n",
        "os.makedirs(PDF_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Configuration set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zoKv2TkKdmTa",
        "outputId": "6f2a042a-c7c5-4e3b-f757-c4edaebd4695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Supabase\n",
            "Citation metadata columns already exist.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Connect to Supabase + add citation metadata columns\n",
        "from supabase import create_client\n",
        "import requests\n",
        "\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "print(\"Connected to Supabase\")\n",
        "\n",
        "# --- Add citation metadata columns if they don't exist ---\n",
        "# We use the Supabase REST endpoint to execute SQL via PostgREST's RPC.\n",
        "# First, create a helper function, then call it, then drop it.\n",
        "\n",
        "MIGRATION_SQL = \"\"\"\n",
        "DO $$\n",
        "BEGIN\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS author_full TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS title_full TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS source_type TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS publisher TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS place TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS journal TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS volume TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS issue TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS pages TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS editors TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS book_title TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS edition TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS doi TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS url TEXT;\n",
        "  ALTER TABLE library_texts ADD COLUMN IF NOT EXISTS isbn TEXT;\n",
        "END\n",
        "$$;\n",
        "\"\"\"\n",
        "\n",
        "# Try to select one of the new columns to check if migration already ran\n",
        "try:\n",
        "    test = supabase.table(\"library_texts\").select(\"doi\").limit(1).execute()\n",
        "    print(\"Citation metadata columns already exist.\")\n",
        "except Exception:\n",
        "    print(\"Citation columns not found. Running migration...\")\n",
        "    # Execute via Supabase SQL endpoint (requires service key)\n",
        "    resp = requests.post(\n",
        "        f\"{SUPABASE_URL}/rest/v1/rpc/\",\n",
        "        headers={\n",
        "            \"apikey\": SUPABASE_KEY,\n",
        "            \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        },\n",
        "        json={}\n",
        "    )\n",
        "    # If the RPC approach doesn't work, print SQL for manual execution\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MANUAL STEP NEEDED: Paste this SQL into your Supabase SQL Editor\")\n",
        "    print(\"Dashboard > SQL Editor > New Query > Paste > Run\")\n",
        "    print(\"=\"*60)\n",
        "    print(MIGRATION_SQL)\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nAfter running the SQL, re-run this cell.\")\n",
        "    raise SystemExit(\"Run the SQL above in Supabase SQL Editor, then re-run this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RXiYCTJbdmTa",
        "outputId": "747166ac-5d23-4eb1-a8bf-4285385f3180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts needing OCR: 79\n",
            "\n",
            "By category:\n",
            "  Cognitive: 2\n",
            "  Early_Modern: 3\n",
            "  Eliot: 10\n",
            "  Medieval: 8\n",
            "  Modernism: 4\n",
            "  Other: 1\n",
            "  Philosophy: 12\n",
            "  Poetics: 9\n",
            "  Semiotics: 5\n",
            "  Theory_Method: 25\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Fetch PDF inventory (only texts not yet OCR'd with marker)\n",
        "from collections import Counter\n",
        "\n",
        "all_pdf_rows = []\n",
        "batch_size = 100\n",
        "offset = 0\n",
        "\n",
        "while True:\n",
        "    response = supabase.table(\"library_texts\") \\\n",
        "        .select(\"id, file_name, storage_path, category, has_pdf, ocr_status\") \\\n",
        "        .eq(\"has_pdf\", True) \\\n",
        "        .neq(\"ocr_status\", \"complete\") \\\n",
        "        .range(offset, offset + batch_size - 1) \\\n",
        "        .execute()\n",
        "\n",
        "    if not response.data:\n",
        "        break\n",
        "\n",
        "    all_pdf_rows.extend(response.data)\n",
        "    offset += batch_size\n",
        "\n",
        "print(f\"Texts needing OCR: {len(all_pdf_rows)}\")\n",
        "\n",
        "if all_pdf_rows:\n",
        "    cat_counts = Counter(r[\"category\"] for r in all_pdf_rows)\n",
        "    print(\"\\nBy category:\")\n",
        "    for cat, count in sorted(cat_counts.items()):\n",
        "        print(f\"  {cat}: {count}\")\n",
        "else:\n",
        "    print(\"All PDFs already processed! Nothing to do.\")\n",
        "\n",
        "# Optional: filter to a specific category (set to None to process all)\n",
        "FILTER_CATEGORY = None  # e.g. \"Eliot\" or None for all\n",
        "\n",
        "if FILTER_CATEGORY and all_pdf_rows:\n",
        "    all_pdf_rows = [r for r in all_pdf_rows if r[\"category\"] == FILTER_CATEGORY]\n",
        "    print(f\"\\nFiltered to {FILTER_CATEGORY}: {len(all_pdf_rows)} texts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mOJPHRRidmTa",
        "outputId": "17e25a2b-97a1-4a4a-faa8-2dcfd371dc75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 79 PDFs from Supabase Storage...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [02:47<00:00,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloaded: 64\n",
            "Errors: 15\n",
            "  - Bell_Interactional_Metalepsis_and_Unnatural_Narratology: no storage_path\n",
            "  - Bell_and_Alber_Ontological_Metalepsis: no storage_path\n",
            "  - Bianchi_The_Feminine_Symptom: no storage_path\n",
            "  - Cushman_The_Fallacy_of_Imitative_Form_Revisited: no storage_path\n",
            "  - Eliot-Phillip_Massinger: no storage_path\n",
            "  - Eliot_Clark_Lectures: no storage_path\n",
            "  - Fludernik_Scene_Shift,_Metalepsis,_and_the_Metaleptic_Mode: no storage_path\n",
            "  - Grierson_Introduction: no storage_path\n",
            "  - Kuhn-Treichel_The_Metalepsis_of_Killing_a_Character_in_Diachronic_Perspective: no storage_path\n",
            "  - Malabou_Plasticity_at_the_Dusk_of_Writing: no storage_path\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Download PDFs from Supabase Storage\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "download_errors = []\n",
        "downloaded = []\n",
        "\n",
        "print(f\"Downloading {len(all_pdf_rows)} PDFs from Supabase Storage...\\n\")\n",
        "\n",
        "for row in tqdm(all_pdf_rows):\n",
        "    storage_path = row.get(\"storage_path\")\n",
        "    if not storage_path:\n",
        "        download_errors.append({\"file_name\": row[\"file_name\"], \"error\": \"no storage_path\"})\n",
        "        continue\n",
        "\n",
        "    local_path = os.path.join(PDF_DIR, os.path.basename(storage_path))\n",
        "\n",
        "    # Skip if already downloaded\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        downloaded.append({\"row\": row, \"local_path\": local_path})\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        data = supabase.storage.from_(BUCKET_NAME).download(storage_path)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "        downloaded.append({\"row\": row, \"local_path\": local_path})\n",
        "    except Exception as e:\n",
        "        download_errors.append({\"file_name\": row[\"file_name\"], \"error\": str(e)})\n",
        "\n",
        "print(f\"\\nDownloaded: {len(downloaded)}\")\n",
        "print(f\"Errors: {len(download_errors)}\")\n",
        "if download_errors:\n",
        "    for err in download_errors[:10]:\n",
        "        print(f\"  - {err['file_name']}: {err['error']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dg9mTp26dmTa",
        "outputId": "3ba2689a-0593-4c37-c796-fa4b33fb8ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.10.0+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'torch._C._CudaDeviceProperties' object has no attribute 'total_mem'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2677942798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU: {torch.cuda.get_device_name(0)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch._C._CudaDeviceProperties' object has no attribute 'total_mem'"
          ]
        }
      ],
      "source": [
        "# Cell 6: Initialize marker\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU\")\n",
        "\n",
        "print(\"\\nLoading marker models (this takes 1-2 minutes)...\")\n",
        "\n",
        "from marker.converters.pdf import PdfConverter\n",
        "from marker.models import create_model_dict\n",
        "\n",
        "model_dict = create_model_dict()\n",
        "converter = PdfConverter(artifact_dict=model_dict)\n",
        "\n",
        "print(\"Marker models loaded and ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KksgPe-8dmTa"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Run marker on all PDFs\n",
        "import re\n",
        "import unicodedata\n",
        "import json\n",
        "import gc\n",
        "\n",
        "# --- Text cleaning functions (from reocr_pipeline.py) ---\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Thorough UTF-8 cleaning for OCR output.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "    ligature_map = {\n",
        "        \"\\ufb00\": \"ff\", \"\\ufb01\": \"fi\", \"\\ufb02\": \"fl\",\n",
        "        \"\\ufb03\": \"ffi\", \"\\ufb04\": \"ffl\", \"\\ufb05\": \"st\", \"\\ufb06\": \"st\",\n",
        "    }\n",
        "    for lig, replacement in ligature_map.items():\n",
        "        text = text.replace(lig, replacement)\n",
        "\n",
        "    quote_map = {\n",
        "        \"\\u2018\": \"'\", \"\\u2019\": \"'\", \"\\u201a\": \"'\", \"\\u201b\": \"'\",\n",
        "        \"\\u201c\": '\"', \"\\u201d\": '\"', \"\\u201e\": '\"', \"\\u201f\": '\"',\n",
        "        \"\\u2032\": \"'\", \"\\u2033\": '\"',\n",
        "        \"\\u2013\": \"-\", \"\\u2014\": \"--\", \"\\u2015\": \"--\",\n",
        "        \"\\u2026\": \"...\", \"\\u00a0\": \" \",\n",
        "        \"\\u200b\": \"\", \"\\u200c\": \"\", \"\\u200d\": \"\", \"\\ufeff\": \"\", \"\\u00ad\": \"\",\n",
        "    }\n",
        "    for char, replacement in quote_map.items():\n",
        "        text = text.replace(char, replacement)\n",
        "\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]', '', text)\n",
        "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
        "    text = re.sub(r'\\\\u[0-9a-fA-F]{0,3}[^0-9a-fA-F]', '', text)\n",
        "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    text = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', text)\n",
        "    text = re.sub(r'\\n\\s*\\d{1,4}\\s*\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n[A-Z][A-Z\\s&]+(\\(\\d{4}\\))\\s+\\d+[:\\.]?\\d*\\s*\\n', '\\n', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    text = re.sub(r'[ \\t]+\\n', '\\n', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def strip_markdown_for_txt(md_text):\n",
        "    \"\"\"Convert markdown to clean plain text.\"\"\"\n",
        "    text = md_text\n",
        "    text = re.sub(r'^#{1,6}\\s+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\*{1,3}([^*]+)\\*{1,3}', r'\\1', text)\n",
        "    text = re.sub(r'_{1,3}([^_]+)_{1,3}', r'\\1', text)\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
        "    text = re.sub(r'!\\[([^\\]]*)\\]\\([^)]+\\)', r'\\1', text)\n",
        "    text = re.sub(r'^[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'```[^`]*```', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'`([^`]+)`', r'\\1', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# --- Subject extraction ---\n",
        "\n",
        "SUBJECT_KEYWORDS = {\n",
        "    \"phenomenology\": [\"phenomenology\", \"phenomenological\", \"husserl\", \"merleau-ponty\", \"heidegger\"],\n",
        "    \"embodiment\": [\"embodied\", \"embodiment\", \"corporeal\", \"somatic\", \"body\"],\n",
        "    \"enactivism\": [\"enactive\", \"enactivism\", \"autopoiesis\"],\n",
        "    \"cognition\": [\"cognitive\", \"cognition\", \"mental representation\", \"working memory\"],\n",
        "    \"emotion\": [\"emotion\", \"affect\", \"feeling\", \"mood\", \"sentiment\"],\n",
        "    \"attention\": [\"attention\", \"attentional\", \"awareness\", \"salience\"],\n",
        "    \"perception\": [\"perception\", \"perceptual\", \"sensory\", \"multisensory\"],\n",
        "    \"metaphysics\": [\"metaphysics\", \"metaphysical\", \"ontology\", \"ontological\"],\n",
        "    \"aesthetics\": [\"aesthetic\", \"beauty\", \"sublime\", \"taste\"],\n",
        "    \"poetics\": [\"poetic\", \"poetry\", \"verse\", \"prosody\", \"meter\", \"rhyme\"],\n",
        "    \"rhetoric\": [\"rhetoric\", \"rhetorical\", \"trope\", \"metaphor\"],\n",
        "    \"modernism\": [\"modernism\", \"modernist\", \"eliot\", \"pound\", \"woolf\"],\n",
        "    \"philosophy\": [\"philosophy\", \"philosophical\", \"epistemology\", \"ethics\"],\n",
        "    \"linguistics\": [\"linguistic\", \"syntax\", \"grammar\", \"morphology\", \"phonology\"],\n",
        "    \"semiotics\": [\"semiotic\", \"sign\", \"signification\", \"peirce\", \"saussure\"],\n",
        "    \"dissociation\": [\"dissociation\", \"dissociated\", \"unified sensibility\"],\n",
        "    \"imagination\": [\"imagination\", \"imaginative\", \"imaginal\", \"fancy\"],\n",
        "    \"narrative\": [\"narrative\", \"narration\", \"narrator\", \"diegesis\", \"story\"],\n",
        "    \"medieval\": [\"medieval\", \"middle ages\", \"chaucer\", \"dante\"],\n",
        "    \"early_modern\": [\"early modern\", \"renaissance\", \"milton\", \"shakespeare\", \"donne\"],\n",
        "    \"neuroscience\": [\"neuroscience\", \"neural\", \"brain\", \"fmri\", \"cortex\", \"hippocampus\"],\n",
        "    \"social_cognition\": [\"social cognition\", \"theory of mind\", \"intersubjectivity\", \"joint attention\"],\n",
        "    \"kant\": [\"kant\", \"kantian\", \"critique of\", \"transcendental\"],\n",
        "    \"disability\": [\"disability\", \"disabled\", \"crip\", \"ableism\", \"accommodation\"],\n",
        "}\n",
        "\n",
        "def extract_subjects(text):\n",
        "    lower = text.lower()\n",
        "    subjects = []\n",
        "    for subject, keywords in SUBJECT_KEYWORDS.items():\n",
        "        for kw in keywords:\n",
        "            if kw in lower:\n",
        "                subjects.append(subject)\n",
        "                break\n",
        "    return list(set(subjects))\n",
        "\n",
        "\n",
        "# --- Process all PDFs ---\n",
        "\n",
        "# Load checkpoint if exists (resume after crash)\n",
        "completed_files = set()\n",
        "results = {}\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    with open(CHECKPOINT_PATH) as f:\n",
        "        checkpoint = json.load(f)\n",
        "    results = checkpoint.get(\"results\", {})\n",
        "    completed_files = set(results.keys())\n",
        "    print(f\"Resuming from checkpoint: {len(completed_files)} already processed\")\n",
        "\n",
        "processing_errors = []\n",
        "total = len(downloaded)\n",
        "\n",
        "print(f\"\\nProcessing {total} PDFs with marker...\\n\")\n",
        "\n",
        "for i, item in enumerate(downloaded):\n",
        "    row = item[\"row\"]\n",
        "    local_path = item[\"local_path\"]\n",
        "    file_name = row[\"file_name\"]\n",
        "\n",
        "    if file_name in completed_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"[{i+1}/{total}] {file_name}\", end=\" ... \")\n",
        "\n",
        "    try:\n",
        "        result = converter(local_path)\n",
        "        md_text = result.markdown\n",
        "\n",
        "        if not md_text or len(md_text.strip()) < 50:\n",
        "            processing_errors.append({\"file_name\": file_name, \"error\": \"empty or near-empty output\"})\n",
        "            print(\"EMPTY\")\n",
        "            continue\n",
        "\n",
        "        plain_text = strip_markdown_for_txt(md_text)\n",
        "        cleaned = clean_text(plain_text)\n",
        "        word_count = len(cleaned.split())\n",
        "        subjects = extract_subjects(cleaned)\n",
        "\n",
        "        output_path = os.path.join(OUTPUT_DIR, file_name + \".txt\")\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(cleaned)\n",
        "\n",
        "        results[file_name] = {\n",
        "            \"id\": row[\"id\"],\n",
        "            \"file_name\": file_name,\n",
        "            \"word_count\": word_count,\n",
        "            \"subjects\": subjects,\n",
        "            \"output_path\": output_path,\n",
        "        }\n",
        "        completed_files.add(file_name)\n",
        "\n",
        "        print(f\"{word_count} words\")\n",
        "\n",
        "    except Exception as e:\n",
        "        processing_errors.append({\"file_name\": file_name, \"error\": str(e)})\n",
        "        print(f\"ERROR: {e}\")\n",
        "\n",
        "    # Checkpoint every 25 files\n",
        "    if (i + 1) % 25 == 0:\n",
        "        with open(CHECKPOINT_PATH, \"w\") as f:\n",
        "            json.dump({\"results\": results, \"errors\": processing_errors}, f)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(f\"  [checkpoint saved: {len(results)} done]\")\n",
        "\n",
        "# Final checkpoint\n",
        "with open(CHECKPOINT_PATH, \"w\") as f:\n",
        "    json.dump({\"results\": results, \"errors\": processing_errors}, f)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Done. {len(results)} processed, {len(processing_errors)} errors.\")\n",
        "if processing_errors:\n",
        "    print(\"\\nErrors:\")\n",
        "    for err in processing_errors:\n",
        "        print(f\"  - {err['file_name']}: {err['error']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SauJEfhdmTb"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Upload cleaned text to Supabase\n",
        "from tqdm import tqdm\n",
        "\n",
        "upload_errors = []\n",
        "uploaded_count = 0\n",
        "\n",
        "print(f\"Uploading {len(results)} processed texts to Supabase...\\n\")\n",
        "\n",
        "for file_name, info in tqdm(results.items()):\n",
        "    try:\n",
        "        with open(info[\"output_path\"], \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        update_data = {\n",
        "            \"content\": content,\n",
        "            \"ocr_status\": \"complete\",\n",
        "            \"word_count\": info[\"word_count\"],\n",
        "            \"subjects\": info[\"subjects\"],\n",
        "            \"embedding\": None,  # Force re-embedding\n",
        "        }\n",
        "\n",
        "        supabase.table(\"library_texts\") \\\n",
        "            .update(update_data) \\\n",
        "            .eq(\"file_name\", file_name) \\\n",
        "            .execute()\n",
        "\n",
        "        uploaded_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        upload_errors.append({\"file_name\": file_name, \"error\": str(e)})\n",
        "\n",
        "print(f\"\\nUploaded: {uploaded_count}\")\n",
        "print(f\"Errors: {len(upload_errors)}\")\n",
        "if upload_errors:\n",
        "    for err in upload_errors[:10]:\n",
        "        print(f\"  - {err['file_name']}: {err['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgV0Qc9EdmTb"
      },
      "source": [
        "---\n",
        "## Citation Metadata Enrichment via CrossRef\n",
        "\n",
        "The cells below query the CrossRef API using each text's author surname + title keywords to retrieve full bibliographic metadata: full author names, full title, journal/publisher, volume/issue/pages, DOI, etc.\n",
        "\n",
        "This covers journal articles and most academic books reliably. Primary texts (poems, literary works) and some older monographs may not resolve and will need manual entry later.\n",
        "\n",
        "Rate: ~1 request/second (polite pool). ~1,060 texts takes ~18 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roZHKBU1dmTb"
      },
      "outputs": [],
      "source": [
        "# Cell 9: CrossRef metadata enrichment\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def query_crossref(author, title, year=None):\n",
        "    \"\"\"\n",
        "    Query CrossRef for bibliographic metadata.\n",
        "    Returns the best-matching work or None.\n",
        "    \"\"\"\n",
        "    params = {\"rows\": 3, \"mailto\": CROSSREF_MAILTO}\n",
        "\n",
        "    # Build query from available fields\n",
        "    if author:\n",
        "        params[\"query.author\"] = author\n",
        "    if title:\n",
        "        params[\"query.bibliographic\"] = title\n",
        "\n",
        "    if not params.get(\"query.author\") and not params.get(\"query.bibliographic\"):\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(\n",
        "            \"https://api.crossref.org/works\",\n",
        "            params=params,\n",
        "            timeout=15\n",
        "        )\n",
        "        if resp.status_code != 200:\n",
        "            return None\n",
        "\n",
        "        data = resp.json()\n",
        "        items = data.get(\"message\", {}).get(\"items\", [])\n",
        "        if not items:\n",
        "            return None\n",
        "\n",
        "        # Score candidates: prefer year match and author match\n",
        "        best = None\n",
        "        best_score = -1\n",
        "\n",
        "        for item in items:\n",
        "            score = item.get(\"score\", 0)\n",
        "\n",
        "            # Bonus for year match\n",
        "            if year:\n",
        "                pub_year = None\n",
        "                for date_field in [\"published-print\", \"published-online\", \"created\"]:\n",
        "                    if date_field in item:\n",
        "                        parts = item[date_field].get(\"date-parts\", [[]])[0]\n",
        "                        if parts:\n",
        "                            pub_year = parts[0]\n",
        "                            break\n",
        "                if pub_year and abs(pub_year - year) <= 1:\n",
        "                    score += 50\n",
        "\n",
        "            # Bonus for author surname match\n",
        "            if author:\n",
        "                cr_authors = item.get(\"author\", [])\n",
        "                for a in cr_authors:\n",
        "                    if a.get(\"family\", \"\").lower() == author.lower():\n",
        "                        score += 30\n",
        "                        break\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best = item\n",
        "\n",
        "        # Require minimum confidence: author surname must appear in results\n",
        "        if best and author:\n",
        "            cr_authors = best.get(\"author\", [])\n",
        "            author_found = any(\n",
        "                a.get(\"family\", \"\").lower() == author.lower()\n",
        "                for a in cr_authors\n",
        "            )\n",
        "            if not author_found:\n",
        "                return None\n",
        "\n",
        "        return best\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_metadata_from_crossref(item):\n",
        "    \"\"\"\n",
        "    Extract citation metadata from a CrossRef work item.\n",
        "    Returns a dict of fields ready for Supabase update.\n",
        "    \"\"\"\n",
        "    meta = {}\n",
        "\n",
        "    # Full author name(s)\n",
        "    authors = item.get(\"author\", [])\n",
        "    if authors:\n",
        "        author_names = []\n",
        "        for a in authors:\n",
        "            given = a.get(\"given\", \"\")\n",
        "            family = a.get(\"family\", \"\")\n",
        "            if given and family:\n",
        "                author_names.append(f\"{given} {family}\")\n",
        "            elif family:\n",
        "                author_names.append(family)\n",
        "        if author_names:\n",
        "            meta[\"author_full\"] = \"; \".join(author_names)\n",
        "\n",
        "    # Full title\n",
        "    titles = item.get(\"title\", [])\n",
        "    subtitles = item.get(\"subtitle\", [])\n",
        "    if titles:\n",
        "        full_title = titles[0]\n",
        "        if subtitles:\n",
        "            full_title += \": \" + subtitles[0]\n",
        "        meta[\"title_full\"] = full_title\n",
        "\n",
        "    # Source type\n",
        "    cr_type = item.get(\"type\", \"\")\n",
        "    type_map = {\n",
        "        \"journal-article\": \"journal_article\",\n",
        "        \"book\": \"book\",\n",
        "        \"book-chapter\": \"book_chapter\",\n",
        "        \"edited-book\": \"edited_volume\",\n",
        "        \"monograph\": \"book\",\n",
        "        \"proceedings-article\": \"journal_article\",\n",
        "        \"dissertation\": \"dissertation\",\n",
        "        \"reference-entry\": \"book_chapter\",\n",
        "    }\n",
        "    meta[\"source_type\"] = type_map.get(cr_type, cr_type)\n",
        "\n",
        "    # Publisher\n",
        "    if item.get(\"publisher\"):\n",
        "        meta[\"publisher\"] = item[\"publisher\"]\n",
        "\n",
        "    # Journal / container title\n",
        "    container = item.get(\"container-title\", [])\n",
        "    if container:\n",
        "        if meta.get(\"source_type\") == \"journal_article\":\n",
        "            meta[\"journal\"] = container[0]\n",
        "        elif meta.get(\"source_type\") in (\"book_chapter\", \"edited_volume\"):\n",
        "            meta[\"book_title\"] = container[0]\n",
        "\n",
        "    # Volume / Issue\n",
        "    if item.get(\"volume\"):\n",
        "        meta[\"volume\"] = item[\"volume\"]\n",
        "    if item.get(\"issue\"):\n",
        "        meta[\"issue\"] = item[\"issue\"]\n",
        "\n",
        "    # Pages\n",
        "    if item.get(\"page\"):\n",
        "        meta[\"pages\"] = item[\"page\"]\n",
        "\n",
        "    # Editors\n",
        "    editors = item.get(\"editor\", [])\n",
        "    if editors:\n",
        "        editor_names = []\n",
        "        for e in editors:\n",
        "            given = e.get(\"given\", \"\")\n",
        "            family = e.get(\"family\", \"\")\n",
        "            if given and family:\n",
        "                editor_names.append(f\"{given} {family}\")\n",
        "            elif family:\n",
        "                editor_names.append(family)\n",
        "        if editor_names:\n",
        "            meta[\"editors\"] = \"; \".join(editor_names)\n",
        "\n",
        "    # DOI\n",
        "    if item.get(\"DOI\"):\n",
        "        meta[\"doi\"] = item[\"DOI\"]\n",
        "\n",
        "    # ISBN\n",
        "    isbns = item.get(\"ISBN\", [])\n",
        "    if isbns:\n",
        "        meta[\"isbn\"] = isbns[0]\n",
        "\n",
        "    # URL (DOI link)\n",
        "    if item.get(\"DOI\"):\n",
        "        meta[\"url\"] = f\"https://doi.org/{item['DOI']}\"\n",
        "\n",
        "    # Year (update if we got a more precise one)\n",
        "    for date_field in [\"published-print\", \"published-online\", \"created\"]:\n",
        "        if date_field in item:\n",
        "            parts = item[date_field].get(\"date-parts\", [[]])[0]\n",
        "            if parts:\n",
        "                meta[\"year\"] = parts[0]\n",
        "                break\n",
        "\n",
        "    # Publication place (CrossRef rarely has this, but check)\n",
        "    if item.get(\"publisher-location\"):\n",
        "        meta[\"place\"] = item[\"publisher-location\"]\n",
        "\n",
        "    return meta\n",
        "\n",
        "\n",
        "# --- Fetch all library texts that need enrichment ---\n",
        "# Uses direct REST API to avoid postgrest version issues\n",
        "\n",
        "print(\"Fetching library texts for metadata enrichment...\")\n",
        "all_texts = []\n",
        "offset = 0\n",
        "\n",
        "headers = {\n",
        "    \"apikey\": SUPABASE_KEY,\n",
        "    \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
        "}\n",
        "\n",
        "while True:\n",
        "    resp = requests.get(\n",
        "        f\"{SUPABASE_URL}/rest/v1/library_texts\",\n",
        "        headers=headers,\n",
        "        params={\n",
        "            \"select\": \"id,file_name,author,title,year,author_full,doi\",\n",
        "            \"author_full\": \"is.null\",\n",
        "            \"offset\": offset,\n",
        "            \"limit\": 100,\n",
        "        },\n",
        "        timeout=30,\n",
        "    )\n",
        "    if resp.status_code != 200:\n",
        "        print(f\"Error fetching texts: {resp.status_code} {resp.text}\")\n",
        "        break\n",
        "    batch = resp.json()\n",
        "    if not batch:\n",
        "        break\n",
        "    all_texts.extend(batch)\n",
        "    offset += 100\n",
        "\n",
        "print(f\"Texts needing metadata enrichment: {len(all_texts)}\")\n",
        "\n",
        "# Load CrossRef checkpoint\n",
        "cr_completed = set()\n",
        "if os.path.exists(CROSSREF_CHECKPOINT):\n",
        "    with open(CROSSREF_CHECKPOINT) as f:\n",
        "        cr_data = json.load(f)\n",
        "    cr_completed = set(cr_data.get(\"completed\", []))\n",
        "    print(f\"Resuming: {len(cr_completed)} already looked up\")\n",
        "\n",
        "# --- Query CrossRef for each text ---\n",
        "\n",
        "enriched = 0\n",
        "not_found = 0\n",
        "cr_errors = []\n",
        "\n",
        "for i, text in enumerate(tqdm(all_texts)):\n",
        "    file_name = text[\"file_name\"]\n",
        "\n",
        "    if file_name in cr_completed:\n",
        "        continue\n",
        "\n",
        "    author = text.get(\"author\")\n",
        "    title = text.get(\"title\")\n",
        "    year = text.get(\"year\")\n",
        "\n",
        "    if not author and not title:\n",
        "        cr_completed.add(file_name)\n",
        "        not_found += 1\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        cr_item = query_crossref(author, title, year)\n",
        "\n",
        "        if cr_item:\n",
        "            meta = extract_metadata_from_crossref(cr_item)\n",
        "            if meta:\n",
        "                # Update via REST API directly\n",
        "                patch_resp = requests.patch(\n",
        "                    f\"{SUPABASE_URL}/rest/v1/library_texts\",\n",
        "                    headers={\n",
        "                        **headers,\n",
        "                        \"Content-Type\": \"application/json\",\n",
        "                        \"Prefer\": \"return=minimal\",\n",
        "                    },\n",
        "                    params={\"id\": f\"eq.{text['id']}\"},\n",
        "                    json=meta,\n",
        "                    timeout=15,\n",
        "                )\n",
        "                if patch_resp.status_code in (200, 204):\n",
        "                    enriched += 1\n",
        "                else:\n",
        "                    cr_errors.append({\"file_name\": file_name, \"error\": f\"Update failed: {patch_resp.status_code}\"})\n",
        "            else:\n",
        "                not_found += 1\n",
        "        else:\n",
        "            not_found += 1\n",
        "\n",
        "        cr_completed.add(file_name)\n",
        "\n",
        "    except Exception as e:\n",
        "        cr_errors.append({\"file_name\": file_name, \"error\": str(e)})\n",
        "\n",
        "    # Rate limit: ~1 request/sec (CrossRef polite pool)\n",
        "    time.sleep(1.0)\n",
        "\n",
        "    # Checkpoint every 50\n",
        "    if (i + 1) % 50 == 0:\n",
        "        with open(CROSSREF_CHECKPOINT, \"w\") as f:\n",
        "            json.dump({\"completed\": list(cr_completed)}, f)\n",
        "        print(f\"  [checkpoint: {enriched} enriched, {not_found} not found]\")\n",
        "\n",
        "# Final checkpoint\n",
        "with open(CROSSREF_CHECKPOINT, \"w\") as f:\n",
        "    json.dump({\"completed\": list(cr_completed)}, f)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"CrossRef enrichment complete.\")\n",
        "print(f\"  Enriched: {enriched}\")\n",
        "print(f\"  Not found: {not_found}\")\n",
        "print(f\"  Errors: {len(cr_errors)}\")\n",
        "if cr_errors:\n",
        "    for err in cr_errors[:10]:\n",
        "        print(f\"  - {err['file_name']}: {err['error']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWBa9Xv3dmTc"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Verification + metadata coverage report\n",
        "import requests\n",
        "\n",
        "headers = {\n",
        "    \"apikey\": SUPABASE_KEY,\n",
        "    \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
        "    \"Prefer\": \"count=exact\",\n",
        "}\n",
        "\n",
        "def count_query(params=None):\n",
        "    \"\"\"Query library_texts with exact count via REST.\"\"\"\n",
        "    resp = requests.get(\n",
        "        f\"{SUPABASE_URL}/rest/v1/library_texts\",\n",
        "        headers=headers,\n",
        "        params={\"select\": \"id\", **(params or {})},\n",
        "        timeout=15,\n",
        "    )\n",
        "    # Count is in the content-range header\n",
        "    cr = resp.headers.get(\"content-range\", \"\")\n",
        "    # Format: \"0-99/1234\" or \"*/1234\"\n",
        "    if \"/\" in cr:\n",
        "        return int(cr.split(\"/\")[1])\n",
        "    return len(resp.json())\n",
        "\n",
        "total = count_query()\n",
        "ocr_complete = count_query({\"ocr_status\": \"eq.complete\"})\n",
        "null_emb = count_query({\"embedding\": \"is.null\"})\n",
        "has_author_full = count_query({\"author_full\": \"not.is.null\"})\n",
        "has_doi = count_query({\"doi\": \"not.is.null\"})\n",
        "has_journal = count_query({\"journal\": \"not.is.null\"})\n",
        "has_publisher = count_query({\"publisher\": \"not.is.null\"})\n",
        "has_source_type = count_query({\"source_type\": \"not.is.null\"})\n",
        "\n",
        "print(f\"Total library texts: {total}\")\n",
        "print(f\"Texts with ocr_status='complete': {ocr_complete}\")\n",
        "print(f\"Texts needing re-embedding: {null_emb}\")\n",
        "\n",
        "print(f\"\\n--- Citation Metadata Coverage ---\")\n",
        "print(f\"  author_full:  {has_author_full}/{total} ({100*has_author_full//total}%)\")\n",
        "print(f\"  doi:          {has_doi}/{total} ({100*has_doi//total}%)\")\n",
        "print(f\"  source_type:  {has_source_type}/{total} ({100*has_source_type//total}%)\")\n",
        "print(f\"  journal:      {has_journal}/{total} ({100*has_journal//total}%)\")\n",
        "print(f\"  publisher:    {has_publisher}/{total} ({100*has_publisher//total}%)\")\n",
        "\n",
        "# Show texts still missing author_full\n",
        "resp = requests.get(\n",
        "    f\"{SUPABASE_URL}/rest/v1/library_texts\",\n",
        "    headers={\n",
        "        \"apikey\": SUPABASE_KEY,\n",
        "        \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
        "    },\n",
        "    params={\n",
        "        \"select\": \"file_name,author,title,year,category\",\n",
        "        \"author_full\": \"is.null\",\n",
        "        \"order\": \"category\",\n",
        "        \"limit\": 30,\n",
        "    },\n",
        "    timeout=15,\n",
        ")\n",
        "missing = resp.json() if resp.status_code == 200 else []\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n--- Sample texts still needing manual metadata ({len(missing)} shown) ---\")\n",
        "    for row in missing:\n",
        "        print(f\"  [{row.get('category','?')}] {row.get('author','?')} - {row.get('title','?')} ({row.get('year','?')})\")\n",
        "\n",
        "print(f\"\\n--- Next Steps ---\")\n",
        "print(f\"1. Run embed_library_colab.ipynb to re-embed all {null_emb} texts\")\n",
        "print(f\"2. Manually fill metadata for texts CrossRef couldn't resolve (primary texts, older monographs)\")"
      ]
    }
  ]
}